# Deployment Plan

Complete implementation plan for deploying a Kubernetes cluster on OCI free tier.

## Infrastructure Configuration

**Region**: uk-london-1  
**Account**: PAYG (Pay-As-You-Go) recommended for better Ampere availability  
**Budget Alert**: $0.01 threshold

### Resource Allocation

3x Ampere A1 nodes (ARM64):
- 1.33 OCPU each (3.99 total)
- 8GB RAM each (24GB total)
- 50GB storage each (150GB total)

1x E2.1.Micro bastion (x86):
- 1GB RAM
- 50GB storage

Total: 4 OCPUs, 24GB RAM, 200GB storage (free tier maxed)

### Networking

- VCN with public subnet, internet gateway
- Security lists: SSH (22), HTTP (80), HTTPS (443), ICMP
- Reserved IP #1: Micro bastion (static SSH access)
- Reserved IP #2: K8s ingress controller
- Ephemeral IPs: Ampere nodes (setup/management)
- Tailscale mesh: Secure internal communication
- Custom OCI DNS zone (avoid autogenerated domain)

## Phase 1: Image Building

Build two custom images using Packer.

### Base Image (`base-hardened.qcow2`)

Source: Debian 12 netinstall

Installed packages:
- SSH server with hardened configuration
- Tailscale
- Essential system utilities
- Firewall (nftables/iptables configured)

Hardening steps:
- Minimal package installation
- SSH key-only authentication
- Firewall rules configured
- Automatic security updates enabled

### Proxmox Image (`proxmox-ampere.qcow2`)

Source: base-hardened.qcow2

Additional packages:
- Proxmox VE (via official installation script)
- Ceph packages: ceph-mon, ceph-osd, ceph-mgr

Configuration:
- Proxmox configured for ARM64
- Prepared for Talos VM deployment
- LXC container support enabled

### Upload to OCI

Commands:
```bash
# Build images
packer build base-hardened.pkr.hcl
packer build -var 'source_image=base-hardened.qcow2' proxmox-ampere.pkr.hcl

# Upload to Object Storage
oci os object put --bucket-name <bucket> --file base-hardened.qcow2
oci os object put --bucket-name <bucket> --file proxmox-ampere.qcow2

# Verify size (must be â‰¤20GB total)
oci os object list --bucket-name <bucket> --query 'data[]."size"' | jq 'add'

# Create custom images
oci compute image create --compartment-id <compartment> \
  --display-name base-hardened \
  --bucket-name <bucket> \
  --object-name base-hardened.qcow2

oci compute image create --compartment-id <compartment> \
  --display-name proxmox-ampere \
  --bucket-name <bucket> \
  --object-name proxmox-ampere.qcow2
```

## Phase 2: Infrastructure Provisioning

Deploy instances using Terraform.

### Terraform Configuration

Update terraform.tfvars:
```hcl
region                     = "uk-london-1"
ampere_instance_count      = 3
ampere_ocpus_per_instance  = 1.33
ampere_memory_per_instance = 8
ampere_boot_volume_size    = 50
micro_instance_count       = 1
micro_boot_volume_size     = 50
```

Add custom images to data.tf or reference OCIDs in main.tf.

Add DNS zone resource:
```hcl
resource "oci_dns_zone" "main" {
  compartment_id = var.compartment_ocid
  name          = "your-domain.com"
  zone_type     = "PRIMARY"
}
```

Add reserved IPs:
```hcl
resource "oci_core_public_ip" "bastion" {
  compartment_id = var.compartment_ocid
  lifetime       = "RESERVED"
  display_name   = "bastion-ip"
}

resource "oci_core_public_ip" "ingress" {
  compartment_id = var.compartment_ocid
  lifetime       = "RESERVED"
  display_name   = "k8s-ingress-ip"
}
```

Deploy:
```bash
cd terraform
terraform init
terraform plan  # Verify only free tier resources
terraform apply
```

### Post-Deployment

Configure Tailscale on all nodes:
```bash
# On each node
tailscale up --authkey <your-key>
```

## Phase 3: Proxmox Cluster and Ceph

Configure Proxmox cluster and distributed storage.

### Form Proxmox Cluster

On first node:
```bash
pvecm create cluster-name
```

On remaining nodes:
```bash
pvecm add <first-node-ip>
```

Verify cluster:
```bash
pvecm status
pvecm nodes
```

### Configure Ceph

Initialize Ceph:
```bash
# On all nodes
pveceph install

# On first node
pveceph init --network <ceph-network-cidr>

# On all nodes
pveceph mon create

# Create OSDs (on all nodes, adjust device names)
pveceph osd create /dev/sdb
```

Create Ceph pool:
```bash
pveceph pool create vm-storage --add_storages
```

Verify Ceph:
```bash
ceph -s
ceph osd tree
ceph health detail
```

### Deploy Tailscale LXC Containers

On each Proxmox node:
```bash
pct create 100 local:vztmpl/debian-12-standard_12.0-1_amd64.tar.zst \
  --hostname tailscale-node1 \
  --memory 256 \
  --rootfs local-lvm:4 \
  --net0 name=eth0,bridge=vmbr0,ip=dhcp

pct start 100
pct enter 100
curl -fsSL https://tailscale.com/install.sh | sh
tailscale up --authkey <your-key>
```

### Test VM Migration

Create test VM:
```bash
qm create 999 --name test-vm --memory 1024 --net0 virtio,bridge=vmbr0
qm set 999 --scsi0 vm-storage:10
qm start 999
```

Test migration (3 methods):

Using qm:
```bash
qm migrate 999 node2 --online
qm status 999
```

Using pvesh:
```bash
pvesh create /nodes/node1/qemu/999/migrate --target node2 --online 1
```

Using Proxmox API:
```bash
curl -k -b /path/to/cookie.txt \
  -X POST "https://proxmox-node:8006/api2/json/nodes/node1/qemu/999/migrate" \
  -d "target=node2" -d "online=1"
```

Automated round-robin test:
```bash
#!/bin/bash
VMID=999
NODES=("node1" "node2" "node3")

for i in {0..2}; do
  SOURCE=${NODES[$i]}
  TARGET=${NODES[$(((i+1)%3))]}
  echo "Migrating VM $VMID: $SOURCE -> $TARGET"
  qm migrate $VMID $TARGET --online
  sleep 30
  qm status $VMID
done
```

Validation:
- VM stays running during migration
- VM responds to ping continuously
- Migration completes in <2 minutes
- Storage accessible from all nodes

## Phase 4: Talos Kubernetes

Deploy Talos Linux VMs and bootstrap K8s cluster.

### Download Talos ISO

```bash
wget https://github.com/siderolabs/talos/releases/download/v1.x.x/metal-arm64.iso
```

Upload to Proxmox:
```bash
scp metal-arm64.iso root@proxmox:/var/lib/vz/template/iso/
```

### Create Talos VMs

On Proxmox (via CLI or API):
```bash
# Control plane node 1
qm create 101 --name talos-cp-1 --memory 4096 --cores 1 --net0 virtio,bridge=vmbr0
qm set 101 --scsi0 vm-storage:20 --boot order=scsi0
qm set 101 --cdrom local:iso/metal-arm64.iso
qm start 101

# Repeat for cp-2 (102), cp-3 (103)
```

### Bootstrap Talos Cluster

Generate configuration:
```bash
talosctl gen config cluster-name https://<control-plane-ip>:6443
```

Apply configuration:
```bash
talosctl apply-config --insecure --nodes <node-ip> --file controlplane.yaml
```

Bootstrap etcd:
```bash
talosctl bootstrap --nodes <first-cp-ip>
```

Get kubeconfig:
```bash
talosctl kubeconfig --nodes <control-plane-ip>
```

### Configure Ingress

Deploy ingress controller:
```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```

Assign reserved IP to ingress service (manually configure in OCI or use LoadBalancer with annotation).

Update DNS records:
```bash
# Via OCI CLI or console
# Point *.your-domain.com to reserved IP #2
```

## Phase 5: Monitoring

Deploy Grafana Cloud agents for observability.

### Grafana Cloud Setup

1. Create account at grafana.com (free tier)
2. Get credentials:
   - Prometheus remote write URL and API key
   - Loki URL and API key
   - Tempo URL and API key (optional)

### Deploy Alloy Agent

Add Helm repo:
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

Create monitoring namespace:
```bash
kubectl create namespace monitoring
```

Deploy Alloy:
```bash
helm install alloy grafana/alloy \
  --namespace monitoring \
  --set config.remoteWrite.url=<prometheus-url> \
  --set config.remoteWrite.username=<grafana-user> \
  --set config.remoteWrite.password=<api-key> \
  --set config.loki.url=<loki-url>
```

### Configure Collection

Edit Alloy config to collect from:
- Proxmox API (host metrics)
- Talos nodes (node-exporter)
- K8s cluster (kube-state-metrics)
- OCI API (resource metrics)
- Tailscale API (mesh status)

### Create Dashboards

In Grafana Cloud, import dashboards for:
- Proxmox VE monitoring
- Ceph cluster health
- Kubernetes cluster overview
- Talos node metrics
- Application logs (Loki)

## Validation Checklist

After each phase:

**Phase 1**:
- [ ] Base image < 10GB
- [ ] Proxmox image < 10GB
- [ ] Total Object Storage < 20GB
- [ ] Custom images created in OCI

**Phase 2**:
- [ ] 3 Ampere + 1 Micro deployed
- [ ] 2 reserved IPs configured
- [ ] Tailscale mesh connected
- [ ] SSH access working
- [ ] Budget alert configured

**Phase 3**:
- [ ] Proxmox cluster quorum (3 nodes)
- [ ] Ceph health OK
- [ ] VM live migration working
- [ ] Tailscale LXC running

**Phase 4**:
- [ ] 3 Talos VMs running
- [ ] K8s cluster bootstrapped
- [ ] kubectl access working
- [ ] Ingress controller deployed
- [ ] DNS records configured

**Phase 5**:
- [ ] Alloy agents deployed
- [ ] Metrics flowing to Grafana Cloud
- [ ] Logs flowing to Loki
- [ ] Dashboards showing data
- [ ] Within free tier limits (10k series, 50GB/month)

## Cost Verification

Before going live, verify $0 cost:

```bash
# Check OCI billing
oci usage-api usage summarize-usage --tenancy-id <tenancy> \
  --time-usage-started <date> --time-usage-ended <date> \
  --granularity DAILY

# Check Grafana Cloud usage
# Via Grafana Cloud UI: Settings > Usage

# Verify budget alert is active
oci budgets budget get --budget-id <budget-id>
```

All resources must show $0.00 usage.
