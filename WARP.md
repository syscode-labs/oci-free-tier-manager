# WARP.md

This file provides guidance to WARP (warp.dev) when working with code in this repository.

## Project Overview

This is an OCI (Oracle Cloud Infrastructure) Free Tier management toolkit that maximizes Always Free resources. The project consists of:
- **Python availability checker** (`check_availability.py`) - monitors when free tier compute instances become available
- **Terraform infrastructure** (`terraform/`) - provisions free tier resources with budget alerts
- **Documentation** - comprehensive guides on OCI Always Free resources

**Account Type:** This project works with both:
- **Always Free tier**: No credit card required, but Ampere instances often unavailable
- **PAYG (Pay-As-You-Go)**: Requires credit card, but Always Free resources remain free forever
  - Same free tier limits apply (4 OCPU Ampere, 2 Micro, 200GB storage, etc.)
  - Better Ampere A1 instance availability (much easier to provision)
  - Budget alerts ($0.01 threshold) protect against accidental charges
  - Recommended if you can't get Always Free tier capacity

**Important:** Always Free resources are identical in both account types - they never expire and cost $0. PAYG just gives better capacity availability.

## Essential Commands

### Availability Checker
```bash
# Run availability check (must have OCI CLI configured)
./check_availability.py

# Set up periodic monitoring (cron)
*/30 * * * * /path/to/check_availability.py >> /path/to/availability.log 2>&1
```

### Terraform Workflow
```bash
# Initialize (first time only)
cd terraform
terraform init

# Preview changes
terraform plan

# Deploy infrastructure
terraform apply

# Destroy all resources
terraform destroy

# Format and validate
terraform fmt
terraform validate

# View outputs
terraform output
terraform output -json > outputs.json
```

### Prerequisites Check
```bash
# Verify OCI CLI is configured
oci iam region list

# Verify Terraform is installed
terraform version
```

## Architecture

### Python Availability Checker (`check_availability.py`)
- Uses OCI CLI to query compute capacity reports
- Checks both Ampere A1 (ARM) and E2.1.Micro (AMD) instance availability
- Returns exit code 0 if capacity available, 1 otherwise
- Can be integrated into automation scripts for immediate deployment when capacity opens
- Reads tenancy ID from `~/.oci/config` (hardcoded path at line 149)

**Key functions:**
- `check_ampere_availability()` - queries VM.Standard.A1.Flex capacity
- `check_micro_availability()` - queries VM.Standard.E2.1.Micro capacity
- `get_availability_domains()` - lists all ADs in region

### Terraform Infrastructure (`terraform/`)

**File structure follows standard practices:**
- `main.tf` - primary resource definitions (VCN, compute, storage, budgets)
- `variables.tf` - input variable declarations with validation
- `data.tf` - data sources (availability domains, OS images)
- `outputs.tf` - output values (IPs, SSH commands)
- `terraform.tfvars.example` - example configuration (copy to `terraform.tfvars`)

**Resource architecture:**
1. **Networking layer** - VCN, subnet, internet gateway, route table, security list (SSH/HTTP/HTTPS/ICMP)
2. **Compute layer** - Ampere A1 instances (ARM64, flexible) and E2.1.Micro instances (x86, fixed)
3. **Storage layer** - boot volumes (min 47GB) and optional additional block volumes
4. **Budget layer** - budget alerts at $0.01 threshold to detect any charges

**Free tier limits enforced:**
- Ampere: max 4 OCPUs, 24GB RAM total (distributed across instances)
- Micro: max 2 instances, fixed shape (1/8 OCPU, 1GB RAM each)
- Storage: 200GB total including all boot volumes and block volumes

**Key design decisions:**
- Uses Ubuntu 22.04 for both Ampere and Micro instances
- All instances in availability domain [0] by default
- Public IPs auto-assigned for internet access (ephemeral, not reserved)
- Security list allows ingress on 22, 80, 443, and ICMP
- Lifecycle ignores image source changes to prevent unintended replacements
- Budget targets compartment-level costs
- Custom OCI DNS domain for DNS management (avoid autogenerated default)

**Reserved IPs:** Free tier includes 2 reserved (static) public IPs, but current config uses ephemeral IPs. To create reserved IPs for bastion/ingress, add `oci_core_public_ip` resources and reference them in instance VNICs.

**OCI Domain:** Configure a custom DNS zone using `oci_dns_zone` resource instead of using the default autogenerated domain. This provides better DNS control and cleaner hostnames.

## OCI-Specific Context

### Ampere A1 Availability
Ampere instances are frequently out of capacity. The availability checker is designed to run repeatedly until capacity opens. Strategies:
- Run checker every 15-30 minutes via cron
- Try different regions by changing `region` variable
- Capacity tends to open during off-peak hours

### Storage Calculations
Example: 4 Ampere instances × 47GB boot = 188GB, leaving only 12GB for additional volumes.

### Terraform Variable Validation
All variables include validation blocks. Key validations:
- `ampere_instance_count`: 0-4
- `ampere_ocpus_per_instance`: 1-4 (total must be ≤4)
- `ampere_memory_per_instance`: 1-24 (total must be ≤24)
- `micro_instance_count`: 0-2
- Boot volumes: 47-200 GB minimum

## Common Patterns

### Optimal Instance Configurations

**IMPORTANT:** You cannot deploy 4 Ampere + 2 Micro due to storage limits. Each instance requires minimum 47GB boot volume:
- 4 Ampere + 2 Micro = 282GB total (exceeds 200GB limit)

**Maximum realistic configuration: 4 Ampere instances only** (188GB storage used)
```hcl
ampere_instance_count      = 4
ampere_ocpus_per_instance  = 1
ampere_memory_per_instance = 6
micro_instance_count       = 0
ampere_boot_volume_size    = 47
```

**Recommended for most workloads: 1 powerful Ampere + 2 Micro** (141GB storage used)
```hcl
ampere_instance_count      = 1
ampere_ocpus_per_instance  = 4
ampere_memory_per_instance = 24
micro_instance_count       = 2
ampere_boot_volume_size    = 47
micro_boot_volume_size     = 47
```
This gives you a powerful ARM server plus two x86 instances for utilities/monitoring.

**Balanced: 2 medium Ampere + 2 Micro** (188GB storage used)
```hcl
ampere_instance_count      = 2
ampere_ocpus_per_instance  = 2
ampere_memory_per_instance = 12
micro_instance_count       = 2
ampere_boot_volume_size    = 47
micro_boot_volume_size     = 47
```

**Recommended for K8s (3-node cluster + bastion): 3 Ampere + 1 Micro** (200GB storage, maxed)
```hcl
ampere_instance_count      = 3
ampere_ocpus_per_instance  = 1.33  # Total: 3.99 OCPUs (maxed)
ampere_memory_per_instance = 8     # Total: 24GB RAM (maxed)
micro_instance_count       = 1
ampere_boot_volume_size    = 50    # Total: 200GB storage (maxed)
micro_boot_volume_size     = 50
```

**Architecture reasoning:**
- **3 Ampere nodes**: Provides Proxmox cluster quorum (3 nodes for HA and distributed storage)
  - Each node runs Proxmox VE as hypervisor
  - **Ceph deployed for distributed storage** (enables VM live migration)
  - Proxmox cluster configured for quorum (3 nodes minimum)
  - Talos Linux VMs deployed on Proxmox after cluster setup (K8s etcd quorum separate concern)
  - Tailscale runs as LXC container in each Proxmox host for mesh networking
  - 1.33 OCPUs per node = ~4 OCPUs total (maxed free tier)
  - 8GB RAM per node = 24GB total (maxed free tier)
  - 50GB storage per node = 200GB total with bastion (maxed free tier)
- **1 Micro bastion**: Hardened minimal Linux distro for SSH access and management
  - Not part of K8s cluster (1GB RAM insufficient for K8s)
  - Acts as jump host to access internal cluster nodes
  - Runs Tailscale for secure mesh networking
  - Provides secure entry point into the infrastructure
  - **OS**: Same base as Ampere nodes (Debian 12 or Ubuntu 22.04) for consistency
  - Hardened via cloud-init/Ansible post-deployment
  - Region: **uk-london-1** (closest to UK) or eu-frankfurt-1

**Why common base image:**
- Proxmox requires Debian/Ubuntu (not compatible with DietPi or minimal distros)
- Using same OCI platform image (Debian 12 or Ubuntu 22.04) for all 4 nodes ensures:
  - Proxmox compatibility on Ampere nodes
  - Consistent tooling and package management
  - Simpler deployment (single image source)
- Differentiation via cloud-init: Proxmox install on Ampere, hardening on Micro

**Networking architecture:**
- **Tailscale mesh**: All nodes (bastion + 3 Ampere) connected via Tailscale for secure internal communication
- **Proxmox Tailscale deployment**: Tailscale runs as LXC containers on Proxmox hosts (not directly on host OS)
- **Public access**: Only bastion and OCI reserved IP #2 (via 1:1 NAT) have public IPs
- **CNI**: Cilium kube-proxy-free with L2 announcement and Hubble observability
- **Cloud Controller**: OCI CCM for reserved IP management (optional, 1:1 NAT is simpler)
- **Service mesh**: Tailscale Operator for internal service exposure

**IP allocation strategy (2 reserved IPs):**
1. **Reserved IP #1 → Micro bastion**: Static IP for consistent SSH access
2. **Reserved IP #2 → K8s public services**: Forwarded via 1:1 NAT on Proxmox host to K8s NodePort
   - Proxmox iptables DNAT forwards traffic to Talos VMs
   - Supports 1-2 public services (HTTP/HTTPS)
   - Alternative: Use OCI CCM to attach IP via service annotation
3. **Ephemeral IPs → Ampere nodes**: Used for initial Proxmox access and Tailscale connectivity
4. **Tailscale IPs → Internal services**: Unlimited internal services exposed via Tailscale Operator

### Automated Deployment on Availability
```bash
if ./check_availability.py; then
    cd terraform && terraform apply -auto-approve
fi
```

### SSH Access
After deployment, get SSH commands:
```bash
terraform output ssh_connection_commands
```

## Important Constraints

### Free Tier Limits (Never Exceed These)

1. **Compute:**
   - Ampere A1: 4 OCPUs + 24GB RAM total (flexible)
   - E2.1.Micro: 2 instances max (fixed: 1/8 OCPU, 1GB RAM each)

2. **Storage:**
   - Block volumes: 200GB total (includes ALL boot volumes + block volumes)
   - Object Storage: 20GB total (for custom images)
   - Archive Storage: 10GB total

3. **Networking:**
   - 2 VCNs max
   - 1 Load Balancer (10 Mbps) - **not currently used**
   - 2 reserved public IPs
   - 10 TB/month outbound transfer

4. **Other Services (not currently used, available for future):**
   - 2 Autonomous Databases (20GB each, 1 OCPU each)
   - NoSQL: 133M reads/writes per month, 25GB per table (3 tables max)
   - Logging: 10GB/month
   - Monitoring: 500M ingestion datapoints, 1B retrieval

### Safety Checks

**Before deploying:**
- Verify Terraform plan shows only Always Free resources
- Check total storage: `(ampere_count × ampere_boot_size) + (micro_count × micro_boot_size) ≤ 200GB`
- Check total OCPUs: `ampere_count × ampere_ocpus ≤ 4`
- Check total RAM: `ampere_count × ampere_memory ≤ 24GB`

**After Packer builds:**
- Verify Object Storage usage: `base-hardened.qcow2 + proxmox-ampere.qcow2 ≤ 20GB`
- Check with: `oci os object list --bucket-name <bucket> --query 'data[]."size"' | jq 'add'`

**Budget alert (CRITICAL for PAYG):**
- Set at $0.01 to catch any charges immediately
- Requires valid email in `budget_alert_email` variable
- Budget creation requires tenancy administrator privileges

**OCI CLI authentication:**
- Python script requires OCI CLI configured: `oci setup config`
- Config file expected at `~/.oci/config`
- Terraform requires API key credentials in `terraform.tfvars`

## Custom Image Building with Packer

### Layered Image Strategy

Build images in layers for consistency and reusability:

**Layer 1: Base Image (Debian hardened + SSH + Tailscale)**
```bash
packer build base-hardened.pkr.hcl
```
- Start from Debian 12 netinstall or minimal
- Harden: minimal packages, SSH config, firewall rules, security updates
- Pre-install: Tailscale, essential tools
- Output: `base-hardened.qcow2`

**Layer 2a: Bastion Image (use base as-is)**
- Deploy base image directly to Micro instance
- No additional layers needed

**Layer 2b: Proxmox Image (base + Proxmox + Ceph)**
```bash
packer build -var 'source_image=base-hardened.qcow2' proxmox.pkr.hcl
```
- Start from base hardened image
- Install Proxmox VE via official script
- Run **Proxmox Helper Scripts** (tteck/Proxmox):
  - Disable enterprise repository, enable no-subscription repo
  - Post-install hardening (disable commercial nag, optimize)
  - Optional: CPU governor, IOMMU, kernel tweaks
- Install Ceph packages (ceph-mon, ceph-osd, ceph-mgr)
- Configure for Talos VMs and LXC containers
- Output: `proxmox-ampere.qcow2`

**Proxmox Helper Scripts (tteck/Proxmox):**
- **Post-install script**: `bash -c "$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)"`
  - Removes enterprise repository warnings
  - Configures community repositories
  - Disables subscription nag dialog
  - Updates system packages
- Run during Packer provisioning for consistent base image

**Note:** Ceph will be pre-installed but configured post-deployment when the Proxmox cluster is formed.

### Proxmox VE Hardening and Configuration

**Essential Proxmox Helper Scripts (tteck/Proxmox):**

The [tteck/Proxmox repository](https://github.com/tteck/Proxmox) provides essential post-install scripts for production Proxmox deployments:

**1. Post-PVE-Install Script (mandatory)**
```bash
bash -c "$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)"
```
- Disables enterprise repository (requires paid subscription)
- Enables no-subscription (community) repository
- Removes subscription nag dialog on web UI login
- Updates package lists and system
- Corrects APT sources for non-enterprise use

**2. Proxmox VE Processor Microcode (recommended)**
```bash
bash -c "$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/microcode.sh)"
```
- Installs latest CPU microcode for ARM64 (critical for Ampere)
- Improves security and stability
- Patches CPU vulnerabilities (Spectre, Meltdown variants)

**3. Kernel Clean-Up (optional, saves space)**
```bash
bash -c "$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/kernel-clean.sh)"
```
- Removes old kernel versions to free boot partition space
- Important on 50GB boot volumes with limited space
- Keep 2 most recent kernels for safety

**Additional Proxmox hardening steps:**

**Disable unnecessary services:**
```bash
systemctl disable --now pve-ha-lrm pve-ha-crm  # Disable HA if not using
systemctl disable --now corosync pve-cluster    # Disable if single-node
```

**Configure Ceph-specific optimizations (for ARM):**
```bash
# Set appropriate OSD journal size for limited RAM
ceph config set osd osd_memory_target 2147483648  # 2GB per OSD
ceph config set osd osd_journal_size 1024         # 1GB journal
```

**Enable automatic security updates:**
```bash
apt install unattended-upgrades
dpkg-reconfigure -plow unattended-upgrades
```

**Set up email alerts (optional, if SMTP configured):**
```bash
pveum user modify root@pam --email your@email.com
echo "root: your@email.com" >> /etc/aliases
newaliases
```

**Proxmox-specific best practices for OCI free tier:**

1. **Storage optimization:**
   - Use `dir` storage backend for VM images (not LVM on ARM)
   - Enable thin provisioning where possible
   - Configure Ceph with `size=2 min_size=1` for 3-node cluster (allows operation with 1 node down)

2. **Memory management:**
   - Set RAM limits for VMs to prevent OOM on host
   - Reserve 2GB per Proxmox host for Ceph services
   - Use `balloon` device in VMs for dynamic memory allocation

3. **Network configuration:**
   - Bridge VMs through `vmbr0` connected to Tailscale
   - Use Proxmox SDN for advanced networking (optional)
   - Configure firewall rules per-VM in Proxmox UI

4. **Backup strategy (within free tier):**
   - Use Proxmox Backup Server as LXC container (not separate VM)
   - Store backups in Ceph pool with replication
   - Alternatively, backup to OCI Object Storage (20GB free)

**Packer provisioning order:**
1. Base Debian/Ubuntu install
2. Run tteck post-pve-install script
3. Install Proxmox VE packages
4. Run microcode script
5. Install Ceph packages (don't configure yet)
6. Run kernel cleanup
7. Create image snapshot

### Manifest Hosting Strategy

**Problem:** Cilium + OCI CCM + Tailscale Operator manifests exceed Talos 1MB inline manifest limit.

**Solution:** Host manifests in external repository and reference via URL in Talos config.

**Repository structure:**
```
github.com/your-user/k8s-manifests-private/
├── cilium/
│   └── manifest.yaml
├── oci-ccm/
│   └── manifest.yaml
└── tailscale-operator/
    └── manifest.yaml
```

**Generate manifests:**
```bash
# Cilium
helm template cilium cilium/cilium \
  --version 1.16.5 \
  --namespace kube-system \
  --values k8s/cilium/values.yaml \
  > manifests-repo/cilium/manifest.yaml

# OCI CCM (from upstream)
kubectl kustomize github.com/oracle/oci-cloud-controller-manager \
  > manifests-repo/oci-ccm/manifest.yaml

# Tailscale Operator
helm template tailscale-operator tailscale/tailscale-operator \
  --namespace tailscale \
  --set oauth.clientId="$TS_CLIENT_ID" \
  --set oauth.clientSecret="$TS_CLIENT_SECRET" \
  > manifests-repo/tailscale-operator/manifest.yaml
```

**Talos configuration:**
```yaml
cluster:
  externalCloudProvider:
    enabled: true
    manifests:
      - https://raw.githubusercontent.com/your-user/k8s-manifests-private/main/cilium/manifest.yaml
      - https://raw.githubusercontent.com/your-user/k8s-manifests-private/main/oci-ccm/manifest.yaml
      - https://raw.githubusercontent.com/your-user/k8s-manifests-private/main/tailscale-operator/manifest.yaml
```

**Security options:**
1. **Private repo with token:** Use GitHub personal access token in URL
2. **Self-hosted:** Host on bastion via HTTPS with basic auth
3. **Encrypted repo:** Use git-crypt for manifest encryption

**Recommended:** Private repo, regenerate manifests locally, never commit secrets to public repos.

**Deployment Workflow:**
1. Build `base-hardened.qcow2` with Packer
2. Build `proxmox-ampere.qcow2` from base with Packer
3. Upload both to OCI Object Storage (free tier: 20GB)
4. Create custom images via OCI CLI
5. Reference OCIDs in Terraform:
   - Micro bastion: base-hardened image
   - Ampere nodes: proxmox-ampere image

**Benefits:**
- Common hardened base for all nodes
- Proxmox compatibility guaranteed
- Fully reproducible and automated
- Can rebuild either layer independently

### Deployment Phases

The infrastructure is deployed in sequential phases:

**Phase 1: Image Building**
1. Build base-hardened.qcow2 (Debian + SSH + Tailscale)
2. Build proxmox-ampere.qcow2 (base + Proxmox + Ceph packages)
3. Upload to OCI Object Storage
4. Create OCI custom images

**Phase 2: Infrastructure Provisioning**
1. Deploy 3 Ampere instances with proxmox-ampere image
2. Deploy 1 Micro bastion with base-hardened image
3. Configure reserved IPs (bastion + Cilium LoadBalancer)
4. Verify Tailscale connectivity
5. Verify Proxmox web UI accessible (https://node:8006) with enterprise nag disabled

**Phase 3: Proxmox Cluster Setup**
1. Form Proxmox cluster (3 nodes)
2. Verify cluster quorum (requires 3 nodes)
3. Configure Ceph storage:
   - Initialize Ceph monitors on all 3 nodes
   - Create OSDs from available storage
   - Configure Ceph pool for VM storage
4. Verify distributed storage and VM migration capability
5. Test VM live migration programmatically (see Testing section below)

**Phase 4: Talos K8s Deployment**
1. Deploy Talos Linux VMs on Proxmox cluster
2. Bootstrap 3-node K8s cluster with CNI disabled and kube-proxy disabled
3. Install Cilium CNI from external manifest (kube-proxy-free + Hubble)
4. Install OCI Cloud Controller Manager from external manifest (optional)
5. Install Tailscale Operator from external manifest
6. Apply default-deny network policies
7. Configure 1:1 NAT on Proxmox for reserved IP #2 → K8s NodePort
8. Test service exposure: public (via NAT) and internal (via Tailscale)

**Phase 5: Monitoring**
1. Deploy Grafana Alloy agents to K8s cluster
2. Configure Grafana Cloud integration
3. Set up dashboards for Proxmox, Ceph, K8s, Tailscale

**Important:** Proxmox cluster quorum and Ceph must be configured before deploying Talos VMs. K8s etcd quorum is separate and handled by Talos.

### Talos Custom Schematic for GPU Support

**Requirement:** If using GPU passthrough in Proxmox VMs (e.g. for AI/ML workloads), Talos requires NVIDIA drivers and container runtime support.

**Solution:** Build a custom Talos schematic with NVIDIA extensions using the [Talos Image Factory](https://factory.talos.dev/).

#### What is a Schematic?

A Talos schematic is a declarative configuration that defines:
- Base Talos version
- System extensions (drivers, tools, firmware)
- Custom kernel modules

Schematics are built server-side by the Image Factory - no Packer or local builds required.

#### Building a GPU-Enabled Schematic

**Method 1: Using Image Factory API (Recommended)**

1. **Create schematic configuration file (`gpu-schematic.yaml`):**
```yaml
customization:
  systemExtensions:
    officialExtensions:
      - siderolabs/nvidia-container-toolkit
      - siderolabs/nonfree-kmod-nvidia
      - siderolabs/nvidia-fabricmanager  # Only for multi-GPU setups
```

2. **Submit to Image Factory and get schematic ID:**
```bash
# Submit schematic and get ID
curl -X POST --data-binary @gpu-schematic.yaml \
  https://factory.talos.dev/schematics \
  | jq -r '.id'

# Example output: 376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba
```

3. **Download Talos image with schematic:**
```bash
# Set schematic ID from previous step
SCHEMATIC_ID="376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba"
TALOS_VERSION="v1.8.3"  # Use latest stable

# Download installer image (for Proxmox deployment)
curl -LO "https://factory.talos.dev/image/${SCHEMATIC_ID}/${TALOS_VERSION}/nocloud-amd64.raw.xz"

# Or get ISO for initial provisioning
curl -LO "https://factory.talos.dev/image/${SCHEMATIC_ID}/${TALOS_VERSION}/metal-amd64.iso"
```

4. **Verify schematic contents:**
```bash
# Query schematic metadata
curl "https://factory.talos.dev/schematics/${SCHEMATIC_ID}" | jq
```

**Method 2: Using `talosctl` CLI**

```bash
# Generate schematic via talosctl (requires talosctl v1.6+)
talosctl image factory schematic create \
  --extension siderolabs/nvidia-container-toolkit \
  --extension siderolabs/nonfree-kmod-nvidia \
  --output gpu-schematic.yaml

# Get schematic ID
talosctl image factory schematic show gpu-schematic.yaml
```

#### NVIDIA Extensions Explained

**Required extensions:**

1. **`siderolabs/nonfree-kmod-nvidia`** (CRITICAL)
   - Proprietary NVIDIA kernel module driver
   - Provides GPU hardware access
   - Built against Talos kernel version
   - Must match Talos version exactly

2. **`siderolabs/nvidia-container-toolkit`** (CRITICAL)
   - NVIDIA Container Toolkit (formerly nvidia-docker2)
   - Exposes GPUs to containers via containerd
   - Provides `nvidia-ctk` and `nvidia-container-runtime`
   - Required for K8s GPU workloads

3. **`siderolabs/nvidia-fabricmanager`** (Optional)
   - Only needed for multi-GPU NVLink/NVSwitch setups
   - Manages high-speed GPU interconnects
   - Skip for single-GPU configurations

**Extension versioning:**
- Extensions are tied to Talos version
- Image Factory automatically selects compatible versions
- To see available extensions: `curl https://factory.talos.dev/extensions`

#### Proxmox GPU Passthrough Configuration

**Before deploying Talos VMs, configure Proxmox hosts:**

1. **Enable IOMMU (required for GPU passthrough):**
```bash
# Edit GRUB config on Proxmox host
vim /etc/default/grub

# Add to GRUB_CMDLINE_LINUX_DEFAULT:
# For Intel CPUs:
GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt"

# For AMD CPUs (Ampere A1 in OCI):
GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt"

# Update GRUB and reboot
update-grub
reboot
```

2. **Load VFIO modules on boot:**
```bash
# Add to /etc/modules
echo "vfio" >> /etc/modules
echo "vfio_iommu_type1" >> /etc/modules
echo "vfio_pci" >> /etc/modules
echo "vfio_virqfd" >> /etc/modules

update-initramfs -u -k all
```

3. **Identify GPU PCI address:**
```bash
lspci -nn | grep -i nvidia
# Example output: 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation ...
```

4. **Attach GPU to Talos VM (via Proxmox CLI or UI):**
```bash
# Via CLI (replace VM ID and PCI address)
qm set 100 --hostpci0 01:00.0,pcie=1

# Via Proxmox UI: VM → Hardware → Add → PCI Device → Select GPU → Enable PCIe
```

**Note:** OCI Ampere A1 instances do NOT have physical GPUs. This configuration is for:
- Using OCI instances with GPU shapes (if available in future)
- Running this stack on local hardware with GPUs
- Testing GPU workloads in development

#### Talos Machine Config for GPU Workloads

**Add to Talos machine configuration:**
```yaml
machine:
  kernel:
    modules:
      - name: nvidia
      - name: nvidia_uvm
      - name: nvidia_drm
      - name: nvidia_modeset
  sysctls:
    # Increase shared memory for GPU workloads
    kernel.shm_rmid_forced: "0"
```

#### Kubernetes GPU Device Plugin

**After Talos cluster is running, deploy NVIDIA device plugin:**
```bash
# Deploy NVIDIA device plugin (DaemonSet)
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.0/deployments/static/nvidia-device-plugin.yml

# Verify GPUs are detected
kubectl get nodes -o json | jq '.items[].status.capacity'
# Look for: "nvidia.com/gpu": "1"
```

**Test GPU access in pod:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  containers:
  - name: cuda-test
    image: nvidia/cuda:12.6.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
  restartPolicy: Never
```

```bash
kubectl apply -f gpu-test.yaml
kubectl logs gpu-test
# Should show nvidia-smi output with GPU info
```

#### Schematic Update Workflow

**When upgrading Talos version:**

1. **Rebuild schematic for new version:**
```bash
# Same YAML, new Talos version
NEW_VERSION="v1.9.0"
curl -X POST --data-binary @gpu-schematic.yaml \
  https://factory.talos.dev/schematics | jq -r '.id'

# Download new image
SCHEMATIC_ID="<new_id>"
curl -LO "https://factory.talos.dev/image/${SCHEMATIC_ID}/${NEW_VERSION}/nocloud-amd64.raw.xz"
```

2. **Upgrade Talos nodes using new schematic:**
```bash
talosctl upgrade \
  --nodes <node-ip> \
  --image factory.talos.dev/installer/${SCHEMATIC_ID}:${NEW_VERSION}
```

#### Automation: Scripted Schematic Creation

**Example script (`build-talos-gpu-schematic.sh`):**
```bash
#!/bin/bash
set -euo pipefail

TALOS_VERSION="${1:-v1.8.3}"
SCHEMATIC_FILE="gpu-schematic.yaml"
OUTPUT_DIR="./talos-images"

mkdir -p "$OUTPUT_DIR"

# Create schematic configuration
cat > "$SCHEMATIC_FILE" <<EOF
customization:
  systemExtensions:
    officialExtensions:
      - siderolabs/nvidia-container-toolkit
      - siderolabs/nonfree-kmod-nvidia
EOF

# Submit to Image Factory
echo "Submitting schematic to Image Factory..."
SCHEMATIC_ID=$(curl -sSX POST --data-binary @"$SCHEMATIC_FILE" \
  https://factory.talos.dev/schematics | jq -r '.id')

echo "Schematic ID: $SCHEMATIC_ID"

# Download images
echo "Downloading Talos images with GPU support..."
curl -L "https://factory.talos.dev/image/${SCHEMATIC_ID}/${TALOS_VERSION}/nocloud-amd64.raw.xz" \
  -o "${OUTPUT_DIR}/talos-gpu-${TALOS_VERSION}.raw.xz"

echo "Image saved to: ${OUTPUT_DIR}/talos-gpu-${TALOS_VERSION}.raw.xz"
echo "Use this image when creating Talos VMs in Proxmox"

# Save schematic ID for reference
echo "$SCHEMATIC_ID" > "${OUTPUT_DIR}/schematic-id.txt"
```

**Usage:**
```bash
chmod +x build-talos-gpu-schematic.sh
./build-talos-gpu-schematic.sh v1.8.3
```

#### References

- **Talos Image Factory:** https://factory.talos.dev/
- **System Extensions Catalog:** https://github.com/siderolabs/extensions
- **NVIDIA Extensions Docs:** https://github.com/siderolabs/extensions/tree/main/nvidia
- **Talos GPU Guide:** https://www.talos.dev/latest/talos-guides/configuration/nvidia-gpu/

### Testing VM Live Migration

After Ceph is configured, test VM migration programmatically:

**Using Proxmox API (recommended):**
```bash
# Migrate VM 100 from node1 to node2
curl -k -b /path/to/cookie.txt \
  -X POST "https://proxmox-node:8006/api2/json/nodes/node1/qemu/100/migrate" \
  -d "target=node2" \
  -d "online=1"

# Check migration status
curl -k -b /path/to/cookie.txt \
  "https://proxmox-node:8006/api2/json/nodes/node1/qemu/100/status/current"
```

**Using pvesh (Proxmox shell):**
```bash
# Migrate VM to another node
pvesh create /nodes/node1/qemu/100/migrate --target node2 --online 1

# Check migration task
pvesh get /nodes/node1/tasks
```

**Using qm command:**
```bash
# Live migrate VM 100 to node2
qm migrate 100 node2 --online

# Check VM status
qm status 100
```

**Automated test script:**
```bash
#!/bin/bash
# Test migration of a VM between all nodes
VMID=100
NODES=("node1" "node2" "node3")

for i in {0..2}; do
  SOURCE_NODE=${NODES[$i]}
  TARGET_NODE=${NODES[$(((i+1)%3))]}
  echo "Migrating VM $VMID: $SOURCE_NODE -> $TARGET_NODE"
  qm migrate $VMID $TARGET_NODE --online
  sleep 30  # Wait for migration to complete
  qm status $VMID
done
```

**Validation checklist:**
- VM remains running during migration (no downtime)
- VM responds to ping throughout migration
- Migration completes in reasonable time (<2 minutes)
- VM storage accessible from all nodes via Ceph

## Monitoring Stack Deployment

**Final deployment step:** Deploy Grafana Cloud agents to K8s cluster

**Architecture:** Use **Grafana Cloud Free Tier** with local agents only (no self-hosted Grafana/Loki/Prometheus)

### Stack Components

**Grafana Cloud (hosted, free tier):**
- Grafana dashboards and visualization
- Loki for log storage (50GB/month free)
- Prometheus/Mimir for metrics (10k series free)
- Traces storage (50GB/month free)

**Local agents (deployed on Talos K8s):**
1. **Grafana Alloy**: Unified observability agent (DaemonSet)
   - Collects logs, metrics, and traces
   - Replaces separate Promtail + Prometheus agents
2. **Grafana Agent** (alternative): If specific integrations needed

### Architecture

**Data flow:**
- Alloy agents (DaemonSet on K8s) → collect from:
  - Proxmox hosts (via Proxmox API)
  - Talos nodes (node metrics, logs)
  - K8s cluster (kube-state-metrics, pod logs)
  - Tailscale mesh status
  - OCI resources (via OCI API)
- Alloy → remote write to Grafana Cloud:
  - Logs → Grafana Loki (cloud)
  - Metrics → Grafana Prometheus/Mimir (cloud)
  - Traces → Grafana Tempo (cloud)

**Visualization:**
- Grafana Cloud dashboards for:
  - Proxmox host metrics (CPU, RAM, storage, VMs)
  - Talos/K8s cluster health
  - Application logs
  - Network/Tailscale connectivity

### Deployment Approach

**Prerequisites:**
1. Create Grafana Cloud account (free tier)
2. Get Grafana Cloud credentials:
   - Prometheus remote write endpoint + API key
   - Loki endpoint + API key
   - Tempo endpoint + API key (optional)

**Deploy Grafana Alloy (recommended):**
```bash
# Add Grafana Helm repo
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Create namespace
kubectl create namespace monitoring

# Deploy Alloy with Grafana Cloud config
helm install alloy grafana/alloy \
  --namespace monitoring \
  --set config.remoteWrite.url=<GRAFANA_CLOUD_PROMETHEUS_URL> \
  --set config.remoteWrite.username=<GRAFANA_CLOUD_USER> \
  --set config.remoteWrite.password=<GRAFANA_CLOUD_API_KEY> \
  --set config.loki.url=<GRAFANA_CLOUD_LOKI_URL>
```

**Alternative: Deploy Grafana Agent:**
```bash
helm install grafana-agent grafana/grafana-agent \
  --namespace monitoring \
  --values grafana-agent-values.yaml
```

**Grafana Cloud Free Tier Limits:**
- **Metrics**: 10,000 series (Prometheus/Mimir)
- **Logs**: 50GB/month ingestion (Loki)
- **Traces**: 50GB/month ingestion (Tempo)
- **Users**: 3 users
- **Retention**: 14 days (metrics), 14 days (logs)

**Constraints:**
- Stay within Grafana Cloud free tier limits (monitor usage in Grafana Cloud UI)
- Only agents run on K8s cluster (minimal resource usage)
- No local storage needed for metrics/logs (stored in Grafana Cloud)
- Configure sampling/filtering in Alloy to stay within limits

**Cost:** $0 - Grafana Cloud free tier + lightweight agents on OCI free tier K8s cluster

## File Modification Guidelines

### When modifying Terraform files:
- Keep file header comments describing purpose
- Store locals in separate `locals.tf` if complexity warrants (not currently needed)
- Create separate data files for complex data resources (current `data.tf` is appropriately sized)
- Avoid heredocs for policies - use data policy documents instead
- Follow conventional commits when committing changes

### When modifying Python script:
- Maintain docstrings for all functions
- Keep timestamp logging format consistent
- Exit code 0 = capacity available, 1 = not available (critical for scripting)
- Update hardcoded path (`/Users/giovanni/.oci/config`) to use environment variable or parameter for portability
